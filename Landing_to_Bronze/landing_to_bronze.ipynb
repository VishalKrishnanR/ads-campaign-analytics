{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ed84b6-80de-4dcc-8331-e41b4307c7d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ADLS \n",
    "storage_account_name = \"adcampaigndata\"\n",
    "landing_container = \"landing\"\n",
    "bronze_container = \"bronze\"\n",
    "\n",
    "# Base paths\n",
    "landing_path = f\"abfss://{landing_container}@{storage_account_name}.dfs.core.windows.net/\"\n",
    "bronze_path = f\"abfss://{bronze_container}@{storage_account_name}.dfs.core.windows.net/\"\n",
    "checkpoint_path = f\"{bronze_path}/checkpoint/\"\n",
    "\n",
    "# Configure Spark to access ADLS\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "  dbutils.secrets.get(scope=\"adls-creds\", key=\"storage-key\")\n",
    ")\n",
    "\n",
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "print(current_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c807f42f-0dd7-479f-9d4c-0f0bce2a9694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "# Create bronze schema if not exists\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "\n",
    "# for listing all the csv folders in the landing zone\n",
    "csv_folders = dbutils.fs.ls(landing_path)\n",
    "\n",
    "table_folders = {\n",
    "    folder.name.strip(\"/\"): folder.name.strip(\"/\")\n",
    "    for folder in csv_folders\n",
    "    if folder.isDir() and not folder.name.startswith(\"checkpoints\") and not folder.name.startswith(\"_\")\n",
    "}\n",
    "print(\"Folders and tables are defined\")\n",
    "for table_name, folder_name in table_folders.items():\n",
    "    source_path = f\"{landing_path}{folder_name}/\"\n",
    "    bronze_table_path = f\"{bronze_path}{folder_name}/\"\n",
    "    checkpoint_path = f\"{bronze_path}checkpoints/{folder_name}/\"\n",
    "\n",
    "    try:\n",
    "        # Autoloader for incremental data ingestion\n",
    "        query = (\n",
    "            spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", \"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"mergeSchema\", \"true\")  # For schema evolution\n",
    "                .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n",
    "                .load(source_path)\n",
    "                .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "                .withColumn(\"modified_by\", lit(current_user))\n",
    "                .writeStream\n",
    "                .format(\"delta\")\n",
    "                .option(\"checkpointLocation\", checkpoint_path)\n",
    "                .outputMode(\"append\")\n",
    "                .trigger(once=True)  # Run once per job\n",
    "                .start(bronze_table_path)\n",
    "        )\n",
    "\n",
    "        query.awaitTermination()\n",
    "\n",
    "        print(f\"✅ Auto Loader started for table: {table_name}\")\n",
    "        print(f\"    - Source path: {source_path}\")\n",
    "        print(f\"    - Bronze path: {bronze_table_path}\")\n",
    "        print(f\"    - Checkpoint path: {checkpoint_path}\")\n",
    "        print(f\"    - Trigger: Once (for daily scheduling)\")\n",
    "\n",
    "        # Register table in metastore\n",
    "        bronze_table_name = f\"br_{table_name}\"\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS bronze.{bronze_table_name}\n",
    "            USING DELTA\n",
    "            LOCATION '{bronze_table_path}'\n",
    "        \"\"\")\n",
    "        print(f\"✅ Registered Hive table: bronze.{bronze_table_name}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing table '{table_name}': {e}\\n\")\n",
    "\n",
    "bronze_tables = dbutils.fs.ls(f\"abfss://{bronze_container}@{storage_account_name}.dfs.core.windows.net/\")\n",
    "\n",
    "for table in bronze_tables:\n",
    "    if table.name.endswith(\"/\") and \"checkpoints\" not in table.name.lower():\n",
    "        bronze_df = spark.read.format(\"delta\").load(\n",
    "            f\"abfss://{bronze_container}@{storage_account_name}.dfs.core.windows.net/{table.name}\"\n",
    "        )\n",
    "        print(f\"Showing data for table: {table.name}\")\n",
    "        display(bronze_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8975955328903598,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "landing_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
