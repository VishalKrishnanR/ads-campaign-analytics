{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f36660b9-1c7a-4592-bdbf-3251177c40fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "from random import *\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# Define paths\n",
    "storage_account_name = \"adcampaigndata\"\n",
    "silver_container = \"silver\"\n",
    "gold_container = \"gold\"\n",
    "\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "  dbutils.secrets.get(scope=\"adls-creds\", key=\"storage-key\")\n",
    ")\n",
    "\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "\n",
    "\n",
    "silver_base = f\"abfss://{silver_container}@{storage_account_name}.dfs.core.windows.net/\"\n",
    "gold_base = f\"abfss://{gold_container}@{storage_account_name}.dfs.core.windows.net/\"\n",
    "\n",
    "# Define paths to cleaned Silver data (update paths as per your setup)\n",
    "silver_youtube_path = silver_base + \"Youtube_ads\"\n",
    "silver_meta_path =  silver_base + \"Meta_ads\"\n",
    "silver_web_path = silver_base + \"Web_analytics\"\n",
    "\n",
    "# Load silver tables\n",
    "df_youtube = spark.read.format(\"delta\").load(silver_youtube_path)\n",
    "df_meta = spark.read.format(\"delta\").load(silver_meta_path)\n",
    "df_web = spark.read.format(\"delta\").load(silver_web_path)\n",
    "\n",
    "\n",
    "# Upsert function\n",
    "def upsert_to_delta(df, table_name, keys, zorder_cols):\n",
    "    if DeltaTable.isDeltaTable(spark, f\"/mnt/gold/{table_name}\"):\n",
    "        delta_table = DeltaTable.forPath(spark, f\"/mnt/gold/{table_name}\")\n",
    "        update_expr = {col: f\"updates.{col}\" for col in df.columns}\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            df.alias(\"updates\"),\n",
    "            \" AND \".join([f\"target.{k} = updates.{k}\" for k in keys])\n",
    "        ).whenMatchedUpdate(set=update_expr) \\\n",
    "         .whenNotMatchedInsertAll() \\\n",
    "         .execute()\n",
    "\n",
    "        if zorder_cols:\n",
    "            spark.sql(f\"OPTIMIZE gold.{table_name} ZORDER BY ({', '.join(zorder_cols)})\")\n",
    "    else:\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(gold_base + f\"/{table_name}\")\n",
    "        table_path = gold_base + f\"/{table_name}\"\n",
    "        # Register or refresh the table in Hive metastore\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS gold.{table_name}\n",
    "            USING DELTA\n",
    "            LOCATION '{table_path}'\n",
    "        \"\"\")\n",
    "        if zorder_cols:\n",
    "                spark.sql(f\"OPTIMIZE gold.{table_name} ZORDER BY ({', '.join(zorder_cols)})\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
